# 27. Streaming y Large Data Processing

En JavaScript moderno (tanto en navegador como en Node.js), los **Streams** permiten procesar datos **de forma incremental** en lugar de cargarlos todos en memoria. Esto es esencial para:

- Archivos de varios GB.
- Datos en tiempo real.
- APIs que envían información parcial.

## 1. Streams API

La **Streams API** nativa de JavaScript define interfaces para manejar datos en flujo:

- **ReadableStream** → fuente de datos.
- **WritableStream** → destino de datos.
- **TransformStream** → proceso intermedio que modifica los datos.

**Ejemplo básico de lectura:**

```javascript
const response = await fetch("https://example.com/largefile");
const reader = response.body.getReader();
const { value, done } = await reader.read();
```

## 2. Readable/Writable Streams

- **Readable** → recibe datos (ej: descarga de archivo).
- **Writable** → envía datos (ej: subir archivo).

**Ejemplo de escritura:**

```javascript
const writable = new WritableStream({
  write(chunk) {
    console.log("Chunk recibido:", chunk);
  },
});
```

## 3. Transform Streams

Permiten modificar datos en tránsito:

```javascript
const upperCaseTransform = new TransformStream({
  transform(chunk, controller) {
    controller.enqueue(chunk.toUpperCase());
  },
});
```

Se encadenan entre un `Readable` y un `Writable`.

## 4. Backpressure

- Mecanismo para evitar saturar la memoria cuando el **productor** envía datos más rápido de lo que el **consumidor** puede procesar.
- El stream puede **pausar** la lectura hasta que el consumidor libere espacio.

## 5. Chunked Processing

- Procesar datos en **trozos** (chunks) en vez de esperar todo.
- Útil para:
  - Leer líneas de un log en vivo.
  - Procesar un video mientras se descarga.
  - Manejar grandes JSON por partes.

**Ejemplo con `TextDecoderStream`:**

```javascript
const res = await fetch("big.txt");
const stream = res.body.pipeTh;
```
